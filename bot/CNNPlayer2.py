import random
import math
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import math
from bot.mmplayer import minimax, listofpossiblemoves
from copy import deepcopy, copy
from bot.Networks import CNNetwork_preset, CNNetwork_big


def heuristic(game, move):
    return 0


class CNNCache():

    def __init__(self):
        self.states_log = []
        self.rewards_log = []
    def add_states(self, states: torch.Tensor, final_reward: float):
        for index_old, state in enumerate(self.states_log):
            for index_new, new_state in enumerate(states):
                if torch.eq(new_state,state):
                    self.rewards_log[index_old].append(final_reward)
                    states.pop(index_new)

        for new_state in states:
            self.states_log.append(new_state)
            self.rewards_log.append([final_reward])
    def __len__(self):
        return len(self.states_log)

    def get_states_and_values(self):
        # computes the value of the state as the avarage of all its observed final rewards
        values = [sum(x)/len(x) for x in self.rewards_log]
        return self.states_log, values

class StateTargetValuesDataset(Dataset):

    def __init__(self, states, targets):
        self.states = states
        self.targets = targets
        if len(states) != len(targets):
            raise ValueError

    def __len__(self):
        return len(self.states)

    def __getitem__(self, index):
        return self.states[index], self.targets[index]


class CNNPlayer_proximal():

    def __init__(self, size, name, preset=False, to_train=False, load=False, pretraining=False,
                 double_dqn=False, random_move_prob=0.9999, random_move_decrease=0.9997, minimax_prob=0.2):
        # self.last_seen = None
        self.side = None
        self.size = size
        self.to_train = to_train
        self.name = "CNN " + name + " " + str(size)
        self.file = "NNs\\" + self.name.replace(" ", "-") + ".nn"
        self.EMPTY = 0

        # model things
        if preset:
            self.model = CNNetwork_preset(size=self.size)
        else:
            self.model = CNNetwork_big(size=self.size)

        self.optim = torch.optim.RMSprop(self.model.parameters(), lr=0.00025)
        self.loss_fn = nn.MSELoss()
        if load:
            if type(load) == type(""):
                self.model.load_state_dict(torch.load(load))
            else:
                self.model.load_state_dict(torch.load(self.file))
            self.model.eval()

        self.old_network = None
        self.double_dqn = double_dqn

        # reinforcemnt learning params
        self.reward_discount = 0.99
        self.win_value = 1.0
        self.draw_value = 0.0
        self.loss_value = -1.0
        # exploitation vs exploration
        self.random_move_increase = 1.1  # if player lost try to explore mroe
        self.random_move_prob = random_move_prob
        self.random_move_decrease = random_move_decrease
        self.minimax_prob = minimax_prob
        self.pretraining = pretraining
        if not self.to_train:
            self.random_move_prob = 0

        # logs
        self.curr_match_board_log = []
        self.curr_match_move_log = []
        self.curr_match_next_max_log = []  # the q value of the next move
        self.curr_match_values_log = []  # log of the q values generated for the respective baord
        self.curr_match_reward_log = []  # log of the rewards generated by the moves
        self.memory = CNNCache()
        print(self.memory.size)

    def encode(self, state):
        nparray = np.array([
            [(state == self.side).astype(int),
             (state == self.wait).astype(int)]
        ])
        output = torch.tensor(nparray, dtype=torch.float32)
        return output

    def new_game(self, side, other):
        print(self.random_move_prob)
        self.side = side
        self.wait = other
        self.curr_match_board_log = []  # not encoded history of board states
        self.curr_match_move_log = []
        self.curr_match_next_max_log = []  # the q value of the next move
        self.curr_match_values_log = []  # log of the q values generated for the respective baord
        self.curr_match_reward_log = []  # log of the rewards generated by the moves

    def train(self):

    def move(self, game, enemy_move):

        if self.to_train:
            self.curr_match_board_log.append(game.state.copy())
            # calculate punishemnt
            if not self.block_training:
                if self.curr_match_reward_log:
                    self.curr_match_reward_log[-1] -= self.reward(game, enemy_move)

        input = self.encode(game.state)
        if self.double_dqn and self.old_network:
            probs, q_values = self.old_network.probs(input)
        else:
            probs, q_values = self.model.probs(input)
        print(q_values)
        q_values = np.copy(q_values)
        for index, value in enumerate(q_values):
            if not game.islegal(game.indextoxy(index)):
                probs[index] = -1
            elif probs[index] < 0:
                probs[index] = 0.0

        rand_n = np.random.rand(1)
        if self.to_train and (rand_n < self.random_move_prob or self.pretraining):
            move = random.choice(listofpossiblemoves(game))
            rand_n2 = np.random.rand(1)
            if rand_n2 < self.minimax_prob and not self.pretraining:
                move = self.minimax_move(game)
        else:
            print("real move")
            move = np.argmax(probs.numpy())
            move = game.indextoxy(move)

        game.move(move)

        # add reward
        if self.to_train:
            if len(self.curr_match_move_log) > 0:
                self.curr_match_next_max_log.append(q_values[np.argmax(probs.numpy())])
            self.curr_match_move_log.append(game.xytoindex(move))
            self.curr_match_values_log.append(q_values)

            if self.block_training:
                reward = self.block_training(game, move)
                self.curr_match_reward_log.append(reward)
            else:
                self.curr_match_reward_log.append(self.reward(game, move))

        return move

    def reward(self, game, move):

        odmena = 0
        points = [game.leftdiagpoints(move[0], move[1]), game.rightdiagpoints(move[0], move[1]),
                  game.rowpoints(move[0], move[1]), game.columnpoints(move[0], move[1])]
        points.sort(reverse=True)
        for point in points:
            if odmena == 0:
                odmena += point ** 2 / 10
            else:
                odmena += point ** 2 / 40

        return odmena

    def train(self, vysledek, epochs, n_recalls=0):
        if vysledek == self.side:
            reward = self.win_value
        elif vysledek == "0":
            reward = self.draw_value
        else:
            reward = self.loss_value

        if n_recalls < 0:
            n_recalls = 0

        self.curr_match_next_max_log.append(0)
        self.curr_match_reward_log[-1] = reward

        encoded_board = [self.encode(x) for x in self.curr_match_board_log]
        this_match = Match(encoded_board, self.curr_match_move_log, self.curr_match_reward_log, reward)
        # if lost last game and now won, i want the nn to really remember how to win

        # self.train_on_matches([this_match], epochs)
        self.memory.add_match(deepcopy(this_match))

        if not self.pretraining:
            games_from_memory = self.memory.get_random_matches(n_recalls)

            self.train_on_matches(games_from_memory, epochs=epochs)

        if self.to_train and not self.pretraining:
            self.random_move_prob *= self.random_move_decrease

    def minimax_move(self, game):
        print("minimax move")
        move = minimax(game, 3, heuristic)
        return move

    def train_on_matches(self, matches: list, epochs):

        self.old_network = deepcopy(self.model)

        # trains on the game loaded in memory
        if not matches:
            return
        y = []
        X = []
        for match in matches:
            match.generate_values(self.model)
            targets = self.calculate_targets(match)
            for target in targets:
                y.append(target)
            # y = torch.cat([y, self.calculate_targets(match)])
            for board in match.encoded_board_log:
                X.append(board)
            # X = torch.cat([X, self.encode(board)])
        dataset = StateTargetValuesDataset(X, y)
        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
        paramdata = []
        for epoch in range(epochs):

            for batch in dataloader:
                # We run the training step with the recorded inputs and new Q value targets.
                X, y = batch
                X = X.view((-1, 2, 8, 8))
                y_hat = self.model(X)
                # y = y.view(-1, 1)

                loss = self.loss_fn(y_hat, y)

                # Backprop
                self.optim.zero_grad()
                loss.backward()
                self.optim.step()

    def save_model(self):
        torch.save(self.model.state_dict(), self.file)  # .replace(" ","-"))

